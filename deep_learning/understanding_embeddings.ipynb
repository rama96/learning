{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A notebook to visualize and understand how Embedding works and try out some things"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Movies Embedding : torch.Size([8, 5])\n",
      "Parameter containing:\n",
      "tensor([[-1.0224e+00,  7.7765e-01, -1.7292e+00, -6.0225e-01,  1.3205e+00],\n",
      "        [ 5.0167e-01,  4.2531e-01, -7.2910e-01,  1.3605e-01,  1.3146e+00],\n",
      "        [ 5.8751e-02,  5.3420e-01, -1.1324e+00, -5.9317e-01, -1.6992e-03],\n",
      "        [-1.4291e-01, -4.2862e-01, -1.2593e+00, -1.0713e+00,  3.0666e-01],\n",
      "        [-7.0405e-01,  2.3527e-01, -1.4152e+00,  9.1666e-02, -5.3713e-01],\n",
      "        [ 7.1883e-01, -8.1998e-01,  8.1300e-01,  3.8109e-01, -1.3744e+00],\n",
      "        [ 7.9027e-01, -1.0999e+00, -5.9524e-01,  1.4957e+00,  6.2148e-01],\n",
      "        [ 5.3431e-01,  2.9356e-02,  1.3172e-01, -5.4032e-01, -2.8683e-01]],\n",
      "       requires_grad=True)\n",
      "User Embedding : torch.Size([10, 5])\n",
      "Parameter containing:\n",
      "tensor([[-1.7936,  0.8009, -0.1172,  1.0408,  0.1826],\n",
      "        [ 0.3929,  0.6648,  0.5748,  0.1417, -0.1654],\n",
      "        [-0.2205, -0.1150,  0.7574, -0.8701,  0.9573],\n",
      "        [ 0.5695, -0.2393,  1.3662,  0.5835,  0.3833],\n",
      "        [-0.5533, -0.6992, -1.4520,  1.4417,  2.0564],\n",
      "        [-1.4189,  0.4271,  0.0545, -1.4464, -0.3420],\n",
      "        [ 2.3025, -0.0251,  0.8882,  0.3622,  0.1712],\n",
      "        [-0.1203,  2.0447,  1.2617, -0.4762, -1.0556],\n",
      "        [ 1.4856, -0.4412, -0.3950, -1.2358, -1.1809],\n",
      "        [ 0.8311,  0.3144, -0.2283, -0.1084,  0.2368]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "n_users=10\n",
    "n_movies=8\n",
    "n_factors = 5\n",
    "\n",
    "user_factors = torch.nn.Embedding(n_users , n_factors)\n",
    "movie_factors = torch.nn.Embedding(n_movies , n_factors)\n",
    "\n",
    "print(\"Movies Embedding :\" , movie_factors.weight.shape)\n",
    "print(movie_factors.weight)\n",
    "\n",
    "print(\"User Embedding :\",user_factors.weight.shape)\n",
    "print(user_factors.weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### See what goes inside the forward prop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "torch.cat(): Sizes of tensors must match except in dimension 1. Got 10 and 8 in dimension 0 (The offending index is 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/3q/3yf064lx3h78z3p716hl7gbh0000gp/T/ipykernel_2563/530428645.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0muser_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_users\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_users\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mmovie_ids\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_movies\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_movies\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muser_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmovie_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: torch.cat(): Sizes of tensors must match except in dimension 1. Got 10 and 8 in dimension 0 (The offending index is 1)"
     ]
    }
   ],
   "source": [
    "n_entries = 100\n",
    "user_ids = torch.tensor(range(n_users)).reshape(n_users,1)\n",
    "movie_ids =torch.tensor([i for i in range(n_movies)]).reshape(n_movies,1)\n",
    "x = torch.cat((user_ids, movie_ids), 1)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "x[1:2,0] = 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 9, 2, 3, 4, 5, 6, 7, 8, 9])\n",
      "tensor([10, 11, 12, 13, 14, 15, 16, 17, 18, 19])\n"
     ]
    }
   ],
   "source": [
    "print(x[:,0])\n",
    "print(x[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.8690, -0.4804,  1.7336,  1.9536, -0.8210],\n",
       "        [-1.4040,  0.4420, -0.6059,  0.4778, -0.1344],\n",
       "        [-1.3346, -0.9378, -0.0837, -0.3786, -1.6721],\n",
       "        [-1.0683,  1.3972, -0.3341, -0.7199, -0.7841],\n",
       "        [ 0.4318,  1.1973,  1.0880,  1.6442,  0.2318],\n",
       "        [-0.1375,  0.1788, -0.2875,  1.5874, -0.0398],\n",
       "        [-0.1507, -0.0549, -0.6437, -0.1132,  3.0500],\n",
       "        [-2.0243,  1.0269, -0.6970,  2.2008,  1.3098],\n",
       "        [ 2.1442,  0.0438,  1.0159, -0.5385, -0.7016],\n",
       "        [-1.4040,  0.4420, -0.6059,  0.4778, -0.1344]],\n",
       "       grad_fn=<EmbeddingBackward>)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "users = user_factors(x[:,0])\n",
    "users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 1.8690, -0.4804,  1.7336,  1.9536, -0.8210],\n",
       "        [ 0.6193, -0.9607,  1.7446, -0.8447,  0.5813],\n",
       "        [-1.3346, -0.9378, -0.0837, -0.3786, -1.6721],\n",
       "        [-1.0683,  1.3972, -0.3341, -0.7199, -0.7841],\n",
       "        [ 0.4318,  1.1973,  1.0880,  1.6442,  0.2318],\n",
       "        [-0.1375,  0.1788, -0.2875,  1.5874, -0.0398],\n",
       "        [-0.1507, -0.0549, -0.6437, -0.1132,  3.0500],\n",
       "        [-2.0243,  1.0269, -0.6970,  2.2008,  1.3098],\n",
       "        [ 2.1442,  0.0438,  1.0159, -0.5385, -0.7016],\n",
       "        [-1.4040,  0.4420, -0.6059,  0.4778, -0.1344]], requires_grad=True)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_factors.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index out of range in self",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/3q/3yf064lx3h78z3p716hl7gbh0000gp/T/ipykernel_2563/2775503379.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmovies\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmovie_factors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Pet_Projects/learning/deep_learning_env/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Pet_Projects/learning/deep_learning_env/lib/python3.8/site-packages/torch/nn/modules/sparse.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 158\u001b[0;31m         return F.embedding(\n\u001b[0m\u001b[1;32m    159\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m             self.norm_type, self.scale_grad_by_freq, self.sparse)\n",
      "\u001b[0;32m~/Pet_Projects/learning/deep_learning_env/lib/python3.8/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   2041\u001b[0m         \u001b[0;31m# remove once script supports set_grad_enabled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2042\u001b[0m         \u001b[0m_no_grad_embedding_renorm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2043\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale_grad_by_freq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msparse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2044\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2045\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: index out of range in self"
     ]
    }
   ],
   "source": [
    "## Shows out of range since embedding is defined only till n_movies and the inputed is greater\n",
    "movies = movie_factors(x[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0],\n",
       "        [9, 1],\n",
       "        [2, 8],\n",
       "        [3, 3],\n",
       "        [4, 4],\n",
       "        [5, 5],\n",
       "        [6, 6],\n",
       "        [7, 7],\n",
       "        [8, 8],\n",
       "        [9, 9]])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[:,1] = torch.tensor(range(n_movies))\n",
    "## Changing entries of the third row\n",
    "x[2:3,1] = 8\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Look at the thrid row to see the difference \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-0.4689, -0.7337, -0.3200,  0.7721, -0.4577],\n",
       "        [ 0.1933, -0.0187, -1.1271, -1.0587, -0.9241],\n",
       "        [ 0.7221,  0.5769, -1.1651, -1.2481, -0.6516],\n",
       "        [-1.3625,  1.6580, -0.8540,  1.0999, -1.1263],\n",
       "        [ 0.5616, -0.7726, -0.2238,  1.1252, -0.6559],\n",
       "        [ 0.1982,  0.1515,  1.1382,  0.8855,  0.8914],\n",
       "        [-0.8765,  0.8940,  0.5413,  0.2371,  0.4993],\n",
       "        [-0.4090, -0.9693,  1.4013,  0.3986, -1.1957],\n",
       "        [ 0.7221,  0.5769, -1.1651, -1.2481, -0.6516],\n",
       "        [ 0.7599, -2.4699, -0.1512,  0.2937,  1.1726]],\n",
       "       grad_fn=<EmbeddingBackward>)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movies = movie_factors(x[:,1])\n",
    "print(\"Look at the thrid row to see the difference \")\n",
    "movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Look at the thrid row to see the difference \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-0.4689, -0.7337, -0.3200,  0.7721, -0.4577],\n",
       "        [ 0.1933, -0.0187, -1.1271, -1.0587, -0.9241],\n",
       "        [ 0.2076, -1.7650,  1.3562, -0.6355, -0.0279],\n",
       "        [-1.3625,  1.6580, -0.8540,  1.0999, -1.1263],\n",
       "        [ 0.5616, -0.7726, -0.2238,  1.1252, -0.6559],\n",
       "        [ 0.1982,  0.1515,  1.1382,  0.8855,  0.8914],\n",
       "        [-0.8765,  0.8940,  0.5413,  0.2371,  0.4993],\n",
       "        [-0.4090, -0.9693,  1.4013,  0.3986, -1.1957],\n",
       "        [ 0.7221,  0.5769, -1.1651, -1.2481, -0.6516],\n",
       "        [ 0.7599, -2.4699, -0.1512,  0.2937,  1.1726]], requires_grad=True)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Look at the thrid row to see the difference \")\n",
    "movie_factors.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([10, 5]), torch.Size([10, 5]))"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "users.shape , movies.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings = (users*movies).sum(dim = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DotProduct(torch.nn.Module):\n",
    "    def __init__(self,n_users , n_movies ,n_factors):\n",
    "        self.user_factors = torch.nn.Embedding(n_users,n_factors)\n",
    "        self.movies_factors = torch.nn.Embedding(n_movies,n_factors)\n",
    "    \n",
    "    def forward(self,input_x):\n",
    "        users = self.user_factors(input_x[:,0])\n",
    "        movies = self.user_factors(input_x[:,1])\n",
    "        \n",
    "        # Note : users and movies would be of the same shape (n_rows , n_factors) , where n_rows is the number of entires in a batch\n",
    "        \n",
    "        ratings = (users*movies).sum(dim=1)\n",
    "        return ratings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "65f0809578cd98e80ae781922cc105b9b8f2ca8bd967a92f71e8a11a791bc223"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('deep_learning_env': venv)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
